---
title: "Regressão Linear"
author: "Nazareno Andrade"
output: 
  html_notebook:
    theme: readable
    fig_width: 7
    toc: true
    toc_float: true

---

```{r message=FALSE, warning=FALSE}
library(openintro)
library(tidyverse)
library(ggbeeswarm)
library(modelr)
library(broom)
theme_set(theme_bw())
```

# Nosso objetivo

Até aqui, já conhecemos diferentes formas de sumarizar uma variável com uma estatística (ex: média, mediana, desvio padrão). Conhecemos também uma forma de avaliar a força e a direção da relação entre duas variáveis, com coeficientes de correlação. 

Com regressão, queremos dar os seguintes passos adiante:

   * descrever além da força e da direção de uma relação, sua *forma*. Por exemplo, queremos além de dizer que pessoas mais altas em geral pesam mais, dizer o quanto se espera que uma pessoa pese a mais se ela for 20cm mais alta que outra.   
   * descrever a relação entre mais de duas variáveis. Por exemplo, dizer que ao adicionar 1GB de memória a mais em uma VM (máquina virtual) e não mexer no número de vCPUs que ela tem, um servidor web rodando nessa VM conseguirá responder mais 20 req/s e adicionar uma vCPU sem mexer na memória aumenta a taxa de respostas em 40 req/s. 
   
Para descrever a relação entre as variávies dessa forma, escolheremos uma variável na qual nos interessa entender o efeito das demais. Essa será nossa variável de resposta ou saída, ou variável dependente. As demais são variáveis de entrada, preditores ou variáveis independentes. No exemplo acima, taxa de respostas é a variável de resposta, e número de vCPUs e quantidade de memória da VM são variáveis de entrada, ou preditores, ou variáveis dependentes.

## Funções, modelos, famílias

Em regressão, descreveremos a relação entre variáveis como uma função matemática. Por exemplo, uma resposta de nossa análise será algo como uma dessas 3 funções:

  * $reqs = 4 *mem + 10*vCPUs -2$, ou    
  * $reqs = 1.2 *mem^2 + 10*vCPUs -7.1$ ou    
  * $reqs = 2.1 *mem + 10*2^{vCPUs} - 4$   

No primeiro exemplo, a taxa de requisição é uma função linear da quantidade de memória e do número de vCPUs. No segundo caso, a taxa de requisições aumenta com o quadrado do número de GBs de memória no nosso servidor. Isso quer dizer que o efeito de aumentar $mem$ de 0 para 1 ($=1.2*1 = 1.2$ nesse caso) não é o mesmo que o efeito de aumentar $mem$ de 1 para 2 ($=1.2*2^2 - 1.2=3.6$). Essa é a definição de não-linearidade, a propósito. 

Na terceira função na lista, a taxa de requisições cresce exponencialmente com o número de vCPUs. Aumentar $vCPUs$ em uma unidade multiplica por 2 o valor do termo $10*2^{vCPUs}$.

Esses três exemplos dão exemplos de três famílias de funções: lineares, polinomiais e exponenciais. Em regressão construiremos funções para descrever a relação entre variáveis. Normalmente a escolha da família de funções a ser usada é uma escolha do analista. Os métodos de regressão servirão para encontrar os valores dos coeficientes na função que melhor se ajustam aos seus dados. Você depois utilizará estatística para afirmar quão bem a função que o método encontrou representa seus dados. E iterará nesse processo, claro.

# A intuição

Estamos interessados na relação entre 

```{r}
data(countyComplete)

ggplot(countyComplete, aes(x = hs_grad, y = poverty)) + 
  geom_point(alpha = 0.4, size = .5)
```

No olho:

```{r}
ggplot(countyComplete, aes(x = hs_grad, y = poverty)) + 
  geom_point(alpha = 0.4, size = .8) + 
  geom_abline(slope = 0, intercept = 0, color  = "red") 
```

Quantificando a qualidade do modelo:

```{r}
modelo = function(hs_grad, slope, intercept){
  return(slope * hs_grad + intercept)
}

nossas_estimativas = countyComplete %>% 
  select(hs_grad, poverty) %>% 
  mutate(
    segundo_modelo = modelo(hs_grad, 0, 50), 
    residuo = poverty - segundo_modelo, 
    residuo_quad = residuo**2 # para que fique tudo positivo
  )

fit_modelo = nossas_estimativas %>% summarise(sse = sum(residuo_quad)) %>% pull(sse)
```

É mais fácil se tivermos um parâmetro pra comparar esse número. Usaremos o erro de um modelo sem a variável hs_grad: 

```{r}
ggplot(countyComplete, aes(x = "", y = poverty)) + 
  geom_quasirandom(size = .5, width = .2) + 
  geom_point(aes(y = mean(poverty)), color = "red", size = 3)
```


```{r}
usando_media = countyComplete %>% 
  select(hs_grad, poverty) %>% 
  mutate(
    segundo_modelo = mean(poverty), 
    residuo = poverty - segundo_modelo, 
    residuo_quad = residuo**2
  )

fit_media = usando_media %>% summarise(sse = sum(residuo_quad)) %>% pull(sse)
```

Comparando: de quanto é a redução no erro usando nosso modelo comparado com o da média?

```{r}
(fit_media - fit_modelo)/fit_media
```

## Agora de uma forma menos manual

lm  == linear model

```{r}
ggplot(countyComplete, aes(x = hs_grad, y = poverty)) + 
  geom_point(alpha = 0.4) + 
  geom_smooth(method = "lm", se = FALSE)
```



```{r}
mod <- lm(poverty ~ hs_grad, 
          data = countyComplete)

# sintaxe base R:
summary(mod)
confint(mod)

# broom, que acho mais recomendável: 
tidy(mod)
glance(mod) # depois falaremos desse

countyComplete %>% 
  add_predictions(model = mod) %>% # add o que o modelo estima p cada hs_grad
  ggplot(mapping = aes(x = hs_grad, y = poverty)) + 
  geom_point(alpha = 0.4, size = .1) + 
  geom_line(aes(y = pred), colour = "red")  + 
  geom_abline(intercept = 75, slope = -.72, color  = "darkblue") 
```

```{r}
countyComplete %>% 
  add_residuals(model = mod) %>% 
  ggplot(aes(hs_grad, resid)) + 
  geom_point(alpha = .4, size = .5) + 
  geom_hline(yintercept = 0, colour = "blue")
```

## R^2 é a variância da variável de saída explicada pelo modelo

```{r}
# variância de y
var.y2 <- sum((countyComplete$poverty - mean(countyComplete$poverty))^2)
# variância dos resíduos do modelo
var.residuals <- sum(mod$residuals^2)

#calculando e conferindo o R^2
(var.y2 - var.residuals)/var.y2
rsquare(mod, data = countyComplete)

glance(mod)
```

Em outras situações, outras medidas de erro podem ser úteis

```{r}
rmse(mod, countyComplete)
mae(mod, countyComplete)
qae(mod, countyComplete)
```

## Bootstrap para inferência sobre os parâmetros do modelo

Trabalhando com uma amostra, geralmente queremos inferir o intervalo de confiança para os coeficientes do modelo que descreve a relação que estamos modelando *na população* de onde veio nossa amostra. 

### Versão 1

```{r}
library(purrr)
boot <- modelr::bootstrap(mtcars, 100)

models <- map(boot$strap, ~ lm(mpg ~ wt, data = .))
tidied <- map_df(models, broom::tidy, .id = "id")

tidied %>% 
  ggplot(aes(x = estimate)) + 
  geom_histogram(bins = 30) + 
  facet_grid(. ~ term, scale = "free")
```


### Versão 2

```{r}
library(boot)
library(ISLR) # dados
attach(Auto)
```

Usando o pacote `boot` é preciso criar a função que será usada no bootstrap:

```{r}
boot.fn <- function(data, index) {
  return(coef(lm(mpg ~ horsepower, data=Auto, subset = index)))
}
boot.fn(Auto, 1:392)
```

```{r}
regressao.b = boot(Auto, boot.fn, 1000)
# tidy(regressao.b, conf.int = TRUE, conf.method = "perc") tidy(boot.out) parece bugado em 2017-06-13

plot(regressao.b, index=1) # intercept 
plot(regressao.b, index=2) # horsepower
boot.ci(regressao.b, type = "bca", index = 1) 
boot.ci(regressao.b, type = "bca", index = 2)
```

### Opção com outro pacote

```{r}
library("simpleboot")
modelo.simples = lm(mpg ~ horsepower, data = Auto)
modelo.boot = lm.boot(modelo.simples, R = 1000)
summary(modelo.boot)
perc(modelo.boot, c(.025, .975))
```

